import json
import os
import shutil
import tempfile
import uuid
from contextlib import asynccontextmanager
from datetime import datetime
from typing import List, Optional

import firebase_admin
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI, File, HTTPException, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from firebase_admin import credentials, firestore

# --- LangChain Imports ---
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.prompts import PromptTemplate
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.schema import Document
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pydantic import BaseModel

# --- Tavily Search API Client ---
from tavily import TavilyClient

# --- Carga de variables de entorno desde .env ---
load_dotenv()

# --- Inicializaci√≥n de Firebase Admin SDK ---
FIREBASE_CRED_PATH = os.getenv("FIREBASE_CRED_PATH", "firebase.json")
if not firebase_admin._apps:
    try:
        print("üîç [Firebase] Inicializando Firebase Admin SDK...")
        cred = credentials.Certificate(FIREBASE_CRED_PATH)
        firebase_admin.initialize_app(cred)
        print("‚úÖ [Firebase] Firebase Admin SDK inicializado correctamente.")
    except Exception as e:
        print(f"‚ùå [Firebase] Error al inicializar Firebase: {e}")
        raise

db = firestore.client()

# --- Variables Globales y Configuraci√≥n ---
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

PERSIST_DIRECTORY = "./chroma_db_persistente"
CHROMA_COLLECTION_NAME = "guias_de_estudio_collection"

embeddings_model: Optional[GoogleGenerativeAIEmbeddings] = None
vectorstore: Optional[Chroma] = None
tavily_client: Optional[TavilyClient] = None


# --- Ciclo de vida de la aplicaci√≥n (Lifespan) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestiona la inicializaci√≥n de servicios al arrancar la aplicaci√≥n."""
    print("üöÄ [FastAPI] Iniciando la aplicaci√≥n...")
    initialize_services()
    yield
    print("üëã [FastAPI] Apagando la aplicaci√≥n...")


def initialize_services():
    """Inicializa los servicios principales (modelos, vector store, cliente de b√∫squeda)."""
    global embeddings_model, vectorstore, tavily_client
    print("üîç [Servicios] Inicializando servicios clave...")
    if not GOOGLE_API_KEY:
        raise ValueError("La variable de entorno GOOGLE_API_KEY no est√° configurada.")
    if not TAVILY_API_KEY:
        raise ValueError("La variable de entorno TAVILY_API_KEY no est√° configurada.")
    try:
        embeddings_model = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001", google_api_key=GOOGLE_API_KEY
        )
        print("‚úÖ [Embeddings] Modelo de embeddings cargado.")
    except Exception as e:
        print(f"‚ùå [Embeddings] Error al inicializar el modelo de embeddings: {e}")
        raise
    try:
        vectorstore = Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=embeddings_model,
            collection_name=CHROMA_COLLECTION_NAME,
        )
        print(f"‚úÖ [VectorStore] ChromaDB inicializada en '{PERSIST_DIRECTORY}'.")
        print(f"‚ÑπÔ∏è  [VectorStore] Documentos en DB: {vectorstore._collection.count()}")
    except Exception as e:
        print(f"‚ùå [VectorStore] Error al inicializar ChromaDB: {e}")
        raise
    try:
        tavily_client = TavilyClient(api_key=TAVILY_API_KEY)
        print("‚úÖ [Tavily] Cliente de b√∫squeda web Tavily inicializado.")
    except Exception as e:
        print(f"‚ùå [Tavily] Error al inicializar el cliente de Tavily: {e}")
        raise


# --- Inicializaci√≥n de la App FastAPI ---
app = FastAPI(
    title="ü§ñ Agente IA para Gu√≠as de Estudio",
    description="Un agente inteligente para crear res√∫menes y planes de estudio a partir de PDFs y fuentes online.",
    version="2.5.0",  # Versi√≥n con limpieza de JSON
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- Modelos Pydantic ---
class StudyRequest(BaseModel):
    topic: str
    search_queries: Optional[List[str]] = []
    depth: str = "intermedio"


class StudyGuideResponse(BaseModel):
    guide_id: str
    topic: str
    summary: str
    study_plan: str
    sources_used: List[str]


# --- Funciones de Procesamiento de Documentos ---
def process_and_load_pdfs(files: List[UploadFile]) -> List[Document]:
    """Procesa archivos PDF subidos, extrayendo su contenido."""
    print(f"üìÑ [PDF] Procesando {len(files)} archivo(s) PDF...")
    documents = []
    for file in files:
        if not file.filename.endswith(".pdf"):
            print(f"‚ö†Ô∏è  [PDF] Omitiendo archivo no PDF: {file.filename}")
            continue
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                shutil.copyfileobj(file.file, tmp_file)
                tmp_path = tmp_file.name
            loader = PyPDFLoader(tmp_path)
            pdf_docs = loader.load()
            for doc in pdf_docs:
                doc.metadata["source"] = f"PDF: {file.filename}"
                doc.metadata["type"] = "pdf_upload"
                doc.metadata["upload_time"] = datetime.now().isoformat()
            documents.extend(pdf_docs)
        except Exception as e:
            print(f"‚ùå [PDF] Error al procesar {file.filename}: {e}")
        finally:
            file.file.close()
            if "tmp_path" in locals() and os.path.exists(tmp_path):
                os.unlink(tmp_path)
    print(f"‚úÖ [PDF] {len(documents)} documentos extra√≠dos de los PDFs.")
    return documents


def search_online_sources(queries: List[str], max_results: int = 7) -> List[Document]:
    """Realiza b√∫squedas en la web usando Tavily."""
    if not tavily_client:
        raise ValueError("El cliente de Tavily no est√° inicializado.")
    print(f"üåê [WebSearch] Buscando en la web para {len(queries)} consulta(s)...")
    all_documents = []
    for query in queries:
        try:
            response = tavily_client.search(
                query=query, search_depth="advanced", max_results=max_results
            )
            if "results" in response:
                for result in response["results"]:
                    doc = Document(
                        page_content=result.get("content", ""),
                        metadata={
                            "source": result.get("url", "fuente desconocida"),
                            "title": result.get("title", "T√≠tulo Desconocido"),
                            "type": "web_search",
                            "query": query,
                        },
                    )
                    all_documents.append(doc)
        except Exception as e:
            print(f"‚ùå [WebSearch] Error al buscar '{query}': {e}")
    print(f"‚úÖ [WebSearch] {len(all_documents)} documentos encontrados en la web.")
    return all_documents


def split_documents_into_chunks(documents: List[Document]) -> List[Document]:
    """Divide documentos en fragmentos m√°s peque√±os."""
    if not documents:
        return []
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=300,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""],
    )
    return text_splitter.split_documents(documents)


def add_documents_to_vectorstore(documents: List[Document]):
    """A√±ade documentos a la base de datos de vectores."""
    if not vectorstore:
        raise ValueError("La base de datos de vectores no est√° inicializada.")
    if not documents:
        return 0
    chunks = split_documents_into_chunks(documents)
    if not chunks:
        return 0
    print(f"‚ûï [VectorStore] A√±adiendo {len(chunks)} nuevos fragmentos a ChromaDB...")
    vectorstore.add_documents(chunks)
    return len(chunks)


# --- L√≥gica Principal del Agente (Generaci√≥n de Gu√≠a) ---
def get_llm():
    """Inicializa y devuelve el modelo de lenguaje de Google (Gemini)."""
    return GoogleGenerativeAI(
        model="gemini-1.5-flash",
        google_api_key=GOOGLE_API_KEY,
        temperature=0.6,
        max_output_tokens=8192,
        convert_system_message_to_human=True,
        model_kwargs={"response_mime_type": "application/json"},
    )


def generate_study_guide(topic: str, base_retriever, llm, depth: str) -> dict:
    """Orquesta el proceso RAG para crear la gu√≠a, esperando una respuesta JSON."""
    print(
        f"üß† [RAG] Iniciando generaci√≥n de gu√≠a para '{topic}' (profundidad: {depth})."
    )

    retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm)

    prompt_template = """
    Eres un tutor experto en IA. Tu √∫nica tarea es generar una gu√≠a de estudio en espa√±ol basada en el contexto proporcionado.
    Tu respuesta DEBE ser un √∫nico objeto JSON v√°lido y nada m√°s. No incluyas texto antes o despu√©s del objeto JSON, ni uses marcadores como ```json.

    **Contexto de Informaci√≥n:**
    {context}

    **Instrucciones:**
    A partir del contexto, crea una gu√≠a de estudio sobre "{input}" para un nivel de conocimiento '{depth}'.
    Genera un objeto JSON con la siguiente estructura exacta:
    {{
      "summary": "...",
      "study_plan": "..."
    }}

    **Requisitos del Contenido:**
    1.  **summary**: Un resumen detallado y extenso (m√≠nimo 3 p√°rrafos largos) sobre el tema, explicando los conceptos clave con claridad y profundidad.
    2.  **study_plan**: Un plan de estudio accionable en formato Markdown. Debe dividirse en 3 a 5 m√≥dulos. Cada m√≥dulo debe incluir:
        - T√≠tulo del m√≥dulo (ej. "### M√≥dulo 1: Introducci√≥n a...").
        - **Objetivos**: Lista de 2-3 metas de aprendizaje.
        - **Temas Clave**: Lista de conceptos a estudiar.
        - **Actividades Pr√°cticas**: Lista de 2-3 tareas concretas para aplicar el conocimiento.
    """
    prompt = PromptTemplate(
        template=prompt_template, input_variables=["context", "input", "depth"]
    )

    document_chain = create_stuff_documents_chain(llm, prompt)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    print("   -> üí¨ [RAG] Invocando la cadena de generaci√≥n (esperando JSON)...")
    response = retrieval_chain.invoke({"input": topic, "depth": depth})
    print("   -> ‚úÖ [RAG] Respuesta recibida del LLM.")

    sources_used = {
        doc.metadata.get("source", "Fuente desconocida")
        for doc in response.get("context", [])
    }

    # ‚ú® MEJORA DEFINITIVA: Limpiar y decodificar la respuesta JSON de forma segura
    try:
        answer_text = response.get("answer", "{}")

        # Si la respuesta ya es un diccionario, la usamos directamente
        if isinstance(answer_text, dict):
            guide_json = answer_text
        else:
            # Limpiamos los marcadores de c√≥digo de Markdown si existen
            if "```json" in answer_text:
                # Extraemos el contenido JSON de dentro de los marcadores
                json_str = answer_text.split("```json\n", 1)[1].rsplit("\n```", 1)[0]
            else:
                json_str = answer_text

            # Eliminamos cualquier espacio en blanco o nueva l√≠nea al principio/final
            json_str = json_str.strip()

            # Decodificamos el string JSON limpio
            guide_json = json.loads(json_str)

        return {
            "summary": guide_json.get(
                "summary", "El resumen no pudo ser generado por el modelo."
            ),
            "study_plan": guide_json.get(
                "study_plan", "El plan de estudio no pudo ser generado por el modelo."
            ),
            "sources_used": sorted(list(sources_used)),
        }
    except (json.JSONDecodeError, TypeError, KeyError, IndexError) as e:
        print(
            f"‚ùå [RAG] Error al decodificar o procesar la respuesta JSON del LLM: {e}"
        )
        print(f"   -> Respuesta cruda recibida: {response.get('answer', 'N/A')}")
        # Devolvemos una estructura de error consistente
        return {
            "summary": "Error: La respuesta del modelo no tuvo el formato JSON esperado.",
            "study_plan": f"El modelo devolvi√≥ una respuesta inv√°lida. Contenido recibido:\n\n{response.get('answer', 'No se recibi√≥ contenido.')}",
            "sources_used": sorted(list(sources_used)),
        }


# --- Endpoints de la API ---
@app.get("/")
async def root():
    return {
        "message": "Bienvenido al Agente IA para Gu√≠as de Estudio",
        "status": "operativo",
    }


@app.get("/status")
async def get_status():
    doc_count = vectorstore._collection.count() if vectorstore else "no inicializado"
    return {
        "status": "operativo",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "vectorstore_status": "inicializado" if vectorstore else "no inicializado",
            "documents_in_db": doc_count,
            "google_api_configured": bool(GOOGLE_API_KEY),
            "tavily_api_configured": bool(TAVILY_API_KEY),
        },
    }


@app.post("/upload-pdfs/")
async def upload_pdfs_endpoint(files: List[UploadFile] = File(...)):
    if not files:
        raise HTTPException(status_code=400, detail="No se enviaron archivos.")
    pdf_files = [f for f in files if f.filename.endswith(".pdf")]
    if not pdf_files:
        raise HTTPException(
            status_code=400, detail="Ninguno de los archivos es un PDF v√°lido."
        )
    try:
        documents = process_and_load_pdfs(pdf_files)
        if not documents:
            raise HTTPException(
                status_code=400, detail="No se pudo extraer contenido de los PDFs."
            )
        docs_added = add_documents_to_vectorstore(documents)
        upload_id = f"upload_{uuid.uuid4().hex}"
        db.collection("uploads_log").document(upload_id).set(
            {
                "timestamp": datetime.now(),
                "files_processed": [f.filename for f in pdf_files],
                "documents_added_to_db": docs_added,
            }
        )
        return {
            "status": "√©xito",
            "message": f"{len(pdf_files)} PDF(s) procesados y a√±adidos.",
            "upload_id": upload_id,
            "chunks_added": docs_added,
        }
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error interno al procesar PDFs: {str(e)}"
        )


def generate_guide_background_task(guide_id: str, request: StudyRequest):
    """Tarea en segundo plano para generar la gu√≠a."""
    print(f"‚è≥ [BackgroundTask] Iniciando tarea para la gu√≠a: {guide_id}")
    try:
        if not vectorstore or not tavily_client:
            raise RuntimeError(
                "Los servicios (VectorStore/Tavily) no se inicializaron correctamente."
            )

        if request.search_queries:
            search_docs = search_online_sources(request.search_queries)
            add_documents_to_vectorstore(search_docs)
        elif vectorstore._collection.count() == 0:
            print(
                f"‚ö†Ô∏è  [BackgroundTask] DB vac√≠a. Buscando en web para '{request.topic}'."
            )
            default_queries = [
                f"gu√≠a completa de {request.topic}",
                f"conceptos clave de {request.topic}",
            ]
            search_docs = search_online_sources(default_queries)
            add_documents_to_vectorstore(search_docs)

        llm = get_llm()
        base_retriever = vectorstore.as_retriever(
            search_type="similarity", search_kwargs={"k": 20}
        )
        guide_data = generate_study_guide(
            request.topic, base_retriever, llm, request.depth
        )

        db.collection("study_guides").document(guide_id).update(
            {
                "status": "completado",
                "summary": guide_data["summary"],
                "study_plan": guide_data["study_plan"],
                "sources_used": guide_data["sources_used"],
                "completed_at": datetime.now(),
            }
        )
        print(f"‚úÖ [BackgroundTask] Tarea completada para la gu√≠a: {guide_id}")

    except Exception as e:
        print(f"‚ùå [BackgroundTask] Error en tarea para gu√≠a {guide_id}: {e}")
        db.collection("study_guides").document(guide_id).update(
            {
                "status": "error",
                "error_message": str(e),
                "completed_at": datetime.now(),
            }
        )


@app.post("/generate-study-guide-async/")
async def generate_study_guide_async_endpoint(
    request: StudyRequest, background_tasks: BackgroundTasks
):
    print(f"‚ö°Ô∏è [API-Async] Solicitud para generar gu√≠a sobre: '{request.topic}'")
    guide_id = f"guide_{uuid.uuid4().hex}"
    db.collection("study_guides").document(guide_id).set(
        {
            "status": "procesando",
            "topic": request.topic,
            "depth": request.depth,
            "search_queries": request.search_queries,
            "created_at": datetime.now(),
            "guide_id": guide_id,
        }
    )
    background_tasks.add_task(generate_guide_background_task, guide_id, request)
    return {
        "message": "Solicitud recibida. La gu√≠a se est√° generando en segundo plano.",
        "guide_id": guide_id,
        "status_url": f"/study-guide/{guide_id}",
    }


@app.get(
    "/study-guide/{guide_id}",
    response_model=StudyGuideResponse,
    responses={
        404: {"description": "Gu√≠a no encontrada"},
        202: {"description": "Procesando"},
    },
)
async def get_study_guide_status(guide_id: str):
    """Consulta el estado y resultado de una gu√≠a."""
    doc_ref = db.collection("study_guides").document(guide_id)
    doc = doc_ref.get()

    if not doc.exists:
        raise HTTPException(status_code=404, detail="Gu√≠a no encontrada.")

    data = doc.to_dict()
    status = data.get("status")

    if status == "procesando":
        return JSONResponse(
            status_code=202,
            content={"status": "procesando", "message": "La gu√≠a se est√° procesando."},
        )
    if status == "error":
        raise HTTPException(
            status_code=500,
            detail=f"Error al generar la gu√≠a: {data.get('error_message', 'Desconocido')}",
        )
    if status == "completado":
        return StudyGuideResponse(
            guide_id=guide_id,
            topic=data.get("topic"),
            summary=data.get("summary", ""),
            study_plan=data.get("study_plan", ""),
            sources_used=data.get("sources_used", []),
        )
    raise HTTPException(status_code=500, detail=f"Estado desconocido: {status}")


@app.delete("/admin/reset-knowledge-base/")
async def reset_knowledge_base():
    """[Admin] Borra la base de datos de vectores."""
    print("üö® [Admin] Reiniciando la base de conocimientos...")
    try:
        if os.path.exists(PERSIST_DIRECTORY):
            shutil.rmtree(PERSIST_DIRECTORY)
        initialize_services()
        return {"status": "√©xito", "message": "Base de conocimientos reiniciada."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error al reiniciar: {str(e)}")


if __name__ == "__main__":
    import uvicorn

    print("Iniciando servidor con Uvicorn...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
